{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "import Box2D\n",
    "import numpy as np\n",
    "from Box2D.b2 import (\n",
    "    circleShape,\n",
    "    contactListener,\n",
    "    edgeShape,\n",
    "    fixtureDef,\n",
    "    polygonShape,\n",
    "    revoluteJointDef,\n",
    ")\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces\n",
    "from gym.utils import EzPickle, seeding\n",
    "\n",
    "FPS = 50\n",
    "SCALE = 30.0  # affects how fast-paced the game is, forces should be adjusted as well\n",
    "\n",
    "MAIN_ENGINE_POWER = 13.0\n",
    "SIDE_ENGINE_POWER = 0.6\n",
    "\n",
    "INITIAL_RANDOM = 1000.0  # Set 1500 to make game harder\n",
    "\n",
    "LANDER_POLY = [(-14, +17), (-17, 0), (-17, -10), (+17, -10), (+17, 0), (+14, +17)]\n",
    "LEG_AWAY = 20\n",
    "LEG_DOWN = 18\n",
    "LEG_W, LEG_H = 2, 8\n",
    "LEG_SPRING_TORQUE = 40\n",
    "\n",
    "SIDE_ENGINE_HEIGHT = 14.0\n",
    "SIDE_ENGINE_AWAY = 12.0\n",
    "\n",
    "VIEWPORT_W = 600\n",
    "VIEWPORT_H = 800\n",
    "\n",
    "\n",
    "class ContactDetector(contactListener):\n",
    "    def __init__(self, env):\n",
    "        contactListener.__init__(self)\n",
    "        self.env = env\n",
    "\n",
    "    def BeginContact(self, contact):\n",
    "        if (\n",
    "            self.env.lander == contact.fixtureA.body\n",
    "            or self.env.lander == contact.fixtureB.body\n",
    "        ):\n",
    "            self.env.game_over = True\n",
    "        for i in range(2):\n",
    "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
    "                self.env.legs[i].ground_contact = True\n",
    "\n",
    "    def EndContact(self, contact):\n",
    "        for i in range(2):\n",
    "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
    "                self.env.legs[i].ground_contact = False\n",
    "\n",
    "\n",
    "class LunarLander(gym.Env, EzPickle):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": FPS}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        continuous: bool = False,\n",
    "        gravity: float = -10.0,\n",
    "        enable_wind: bool = False,\n",
    "        wind_power: float = 15.0,\n",
    "    ):\n",
    "        EzPickle.__init__(self)\n",
    "\n",
    "        assert (\n",
    "            -12.0 < gravity and gravity < 0.0\n",
    "        ), f\"gravity (current value: {gravity}) must be between -12 and 0\"\n",
    "        self.gravity = gravity\n",
    "\n",
    "        assert (\n",
    "            0.0 < wind_power and wind_power < 20.0\n",
    "        ), f\"wind_power (current value: {wind_power}) must be between 0 and 20\"\n",
    "        self.wind_power = wind_power\n",
    "\n",
    "        self.enable_wind = enable_wind\n",
    "        self.wind_idx = np.random.randint(-9999, 9999)\n",
    "\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.world = Box2D.b2World(gravity=(0, gravity))\n",
    "        self.moon = None\n",
    "        self.lander = None\n",
    "        self.particles = []\n",
    "\n",
    "        self.prev_reward = None\n",
    "\n",
    "        self.continuous = continuous\n",
    "\n",
    "        low = np.array(\n",
    "            [\n",
    "                # these are bounds for position\n",
    "                # realistically the environment should have ended\n",
    "                # long before we reach more than 50% outside\n",
    "                -1.5,\n",
    "                -1.5,\n",
    "                # velocity bounds is 5x rated speed\n",
    "                -5.0,\n",
    "                -5.0,\n",
    "                -math.pi,\n",
    "                -5.0,\n",
    "                -0.0,\n",
    "                -0.0,\n",
    "            ]\n",
    "        ).astype(np.float32)\n",
    "        high = np.array(\n",
    "            [\n",
    "                # these are bounds for position\n",
    "                # realistically the environment should have ended\n",
    "                # long before we reach more than 50% outside\n",
    "                1.5,\n",
    "                1.5,\n",
    "                # velocity bounds is 5x rated speed\n",
    "                5.0,\n",
    "                5.0,\n",
    "                math.pi,\n",
    "                5.0,\n",
    "                1.0,\n",
    "                1.0,\n",
    "            ]\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        # useful range is -1 .. +1, but spikes can be higher\n",
    "        self.observation_space = spaces.Box(low, high)\n",
    "\n",
    "        if self.continuous:\n",
    "            # Action is two floats [main engine, left-right engines].\n",
    "            # Main engine: -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power.\n",
    "            # Left-right:  -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off\n",
    "            self.action_space = spaces.Box(-1, +1, (2,), dtype=np.float32)\n",
    "        else:\n",
    "            # Nop, fire left engine, main engine, right engine\n",
    "            self.action_space = spaces.Discrete(4)\n",
    "\n",
    "    def _destroy(self):\n",
    "        if not self.moon:\n",
    "            return\n",
    "        self.world.contactListener = None\n",
    "        self._clean_particles(True)\n",
    "        self.world.DestroyBody(self.moon)\n",
    "        self.moon = None\n",
    "        self.world.DestroyBody(self.lander)\n",
    "        self.lander = None\n",
    "        self.world.DestroyBody(self.legs[0])\n",
    "        self.world.DestroyBody(self.legs[1])\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        return_info: bool = False,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        self._destroy()\n",
    "        self.world.contactListener_keepref = ContactDetector(self)\n",
    "        self.world.contactListener = self.world.contactListener_keepref\n",
    "        self.game_over = False\n",
    "        self.prev_shaping = None\n",
    "\n",
    "        W = VIEWPORT_W / SCALE\n",
    "        H = VIEWPORT_H / SCALE\n",
    "\n",
    "        # terrain\n",
    "        CHUNKS = 11\n",
    "        height = self.np_random.uniform(0, H / 2, size=(CHUNKS + 1,))\n",
    "        chunk_x = [W / (CHUNKS - 1) * i for i in range(CHUNKS)]\n",
    "        self.helipad_x1 = chunk_x[CHUNKS // 3  - 1]\n",
    "        self.helipad_x2 = chunk_x[CHUNKS // 3  + 1]\n",
    "        self.helipad_y = H / 4\n",
    "        height[CHUNKS // 3  - 2] = self.helipad_y\n",
    "        height[CHUNKS // 3  - 1] = self.helipad_y\n",
    "        height[CHUNKS // 3  + 0] = self.helipad_y\n",
    "        height[CHUNKS // 3  + 1] = self.helipad_y\n",
    "        height[CHUNKS // 3  + 2] = self.helipad_y\n",
    "        smooth_y = [\n",
    "            0.33 * (height[i - 1] + height[i + 0] + height[i + 1])\n",
    "            for i in range(CHUNKS)\n",
    "        ]\n",
    "\n",
    "        self.moon = self.world.CreateStaticBody(\n",
    "            shapes=edgeShape(vertices=[(0, 0), (W, 0)])\n",
    "        )\n",
    "        self.sky_polys = []\n",
    "        for i in range(CHUNKS - 1):\n",
    "            p1 = (chunk_x[i], smooth_y[i])\n",
    "            p2 = (chunk_x[i + 1], smooth_y[i + 1])\n",
    "            self.moon.CreateEdgeFixture(vertices=[p1, p2], density=0, friction=0.1)\n",
    "            self.sky_polys.append([p1, p2, (p2[0], H), (p1[0], H)])\n",
    "\n",
    "        self.moon.color1 = (0.0, 0.0, 0.0)\n",
    "        self.moon.color2 = (0.0, 0.0, 0.0)\n",
    "\n",
    "        initial_y = VIEWPORT_H / SCALE\n",
    "        self.lander = self.world.CreateDynamicBody(\n",
    "            position=(VIEWPORT_W / SCALE / 2, initial_y),\n",
    "            angle=0.0,\n",
    "            fixtures=fixtureDef(\n",
    "                shape=polygonShape(\n",
    "                    vertices=[(x / SCALE, y / SCALE) for x, y in LANDER_POLY]\n",
    "                ),\n",
    "                density=5.0,\n",
    "                friction=0.1,\n",
    "                categoryBits=0x0010,\n",
    "                maskBits=0x001,  # collide only with ground\n",
    "                restitution=0.0,\n",
    "            ),  # 0.99 bouncy\n",
    "        )\n",
    "        self.lander.color1 = (128, 102, 230)\n",
    "        self.lander.color2 = (77, 77, 128)\n",
    "        self.lander.ApplyForceToCenter(\n",
    "            (\n",
    "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
    "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
    "            ),\n",
    "            True,\n",
    "        )\n",
    "\n",
    "        self.legs = []\n",
    "        for i in [-1, +1]:\n",
    "            leg = self.world.CreateDynamicBody(\n",
    "                position=(VIEWPORT_W / SCALE / 2 - i * LEG_AWAY / SCALE, initial_y),\n",
    "                angle=(i * 0.05),\n",
    "                fixtures=fixtureDef(\n",
    "                    shape=polygonShape(box=(LEG_W / SCALE, LEG_H / SCALE)),\n",
    "                    density=1.0,\n",
    "                    restitution=0.0,\n",
    "                    categoryBits=0x0020,\n",
    "                    maskBits=0x001,\n",
    "                ),\n",
    "            )\n",
    "            leg.ground_contact = False\n",
    "            leg.color1 = (128, 102, 230)\n",
    "            leg.color2 = (77, 77, 128)\n",
    "            rjd = revoluteJointDef(\n",
    "                bodyA=self.lander,\n",
    "                bodyB=leg,\n",
    "                localAnchorA=(0, 0),\n",
    "                localAnchorB=(i * LEG_AWAY / SCALE, LEG_DOWN / SCALE),\n",
    "                enableMotor=True,\n",
    "                enableLimit=True,\n",
    "                maxMotorTorque=LEG_SPRING_TORQUE,\n",
    "                motorSpeed=+0.3 * i,  # low enough not to jump back into the sky\n",
    "            )\n",
    "            if i == -1:\n",
    "                rjd.lowerAngle = (\n",
    "                    +0.9 - 0.5\n",
    "                )  # The most esoteric numbers here, angled legs have freedom to travel within\n",
    "                rjd.upperAngle = +0.9\n",
    "            else:\n",
    "                rjd.lowerAngle = -0.9\n",
    "                rjd.upperAngle = -0.9 + 0.5\n",
    "            leg.joint = self.world.CreateJoint(rjd)\n",
    "            self.legs.append(leg)\n",
    "\n",
    "        self.drawlist = [self.lander] + self.legs\n",
    "\n",
    "        if not return_info:\n",
    "            return self.step(np.array([0, 0]) if self.continuous else 0)[0]\n",
    "        else:\n",
    "            return self.step(np.array([0, 0]) if self.continuous else 0)[0], {}\n",
    "\n",
    "    def _create_particle(self, mass, x, y, ttl):\n",
    "        p = self.world.CreateDynamicBody(\n",
    "            position=(x, y),\n",
    "            angle=0.0,\n",
    "            fixtures=fixtureDef(\n",
    "                shape=circleShape(radius=2 / SCALE, pos=(0, 0)),\n",
    "                density=mass,\n",
    "                friction=0.1,\n",
    "                categoryBits=0x0100,\n",
    "                maskBits=0x001,  # collide only with ground\n",
    "                restitution=0.3,\n",
    "            ),\n",
    "        )\n",
    "        p.ttl = ttl\n",
    "        self.particles.append(p)\n",
    "        self._clean_particles(False)\n",
    "        return p\n",
    "\n",
    "    def _clean_particles(self, all):\n",
    "        while self.particles and (all or self.particles[0].ttl < 0):\n",
    "            self.world.DestroyBody(self.particles.pop(0))\n",
    "\n",
    "    def step(self, action):\n",
    "        # Update wind\n",
    "        if self.enable_wind and not (\n",
    "            self.legs[0].ground_contact or self.legs[1].ground_contact\n",
    "        ):\n",
    "            # the function used for wind is tanh(sin(2 k x) + sin(pi k x)),\n",
    "            # which is proven to never be periodic, k = 0.01\n",
    "            wind_mag = (\n",
    "                math.tanh(\n",
    "                    math.sin(0.02 * self.wind_idx)\n",
    "                    + (math.sin(math.pi * 0.01 * self.wind_idx))\n",
    "                )\n",
    "                * self.wind_power\n",
    "            )\n",
    "            self.wind_idx += 1\n",
    "            self.lander.ApplyForceToCenter(\n",
    "                (wind_mag, 0.0),\n",
    "                True,\n",
    "            )\n",
    "\n",
    "        if self.continuous:\n",
    "            action = np.clip(action, -1, +1).astype(np.float32)\n",
    "        else:\n",
    "            assert self.action_space.contains(\n",
    "                action\n",
    "            ), f\"{action!r} ({type(action)}) invalid \"\n",
    "\n",
    "        # Engines\n",
    "        tip = (math.sin(self.lander.angle), math.cos(self.lander.angle))\n",
    "        side = (-tip[1], tip[0])\n",
    "        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
    "\n",
    "        m_power = 0.0\n",
    "        if (self.continuous and action[0] > 0.0) or (\n",
    "            not self.continuous and action == 2\n",
    "        ):\n",
    "            # Main engine\n",
    "            if self.continuous:\n",
    "                m_power = (np.clip(action[0], 0.0, 1.0) + 1.0) * 0.5  # 0.5..1.0\n",
    "                assert m_power >= 0.5 and m_power <= 1.0\n",
    "            else:\n",
    "                m_power = 1.0\n",
    "            # 4 is move a bit downwards, +-2 for randomness\n",
    "            ox = tip[0] * (4 / SCALE + 2 * dispersion[0]) + side[0] * dispersion[1]\n",
    "            oy = -tip[1] * (4 / SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]\n",
    "            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)\n",
    "            p = self._create_particle(\n",
    "                3.5,  # 3.5 is here to make particle speed adequate\n",
    "                impulse_pos[0],\n",
    "                impulse_pos[1],\n",
    "                m_power,\n",
    "            )  # particles are just a decoration\n",
    "            p.ApplyLinearImpulse(\n",
    "                (ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),\n",
    "                impulse_pos,\n",
    "                True,\n",
    "            )\n",
    "            self.lander.ApplyLinearImpulse(\n",
    "                (-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),\n",
    "                impulse_pos,\n",
    "                True,\n",
    "            )\n",
    "\n",
    "        s_power = 0.0\n",
    "        if (self.continuous and np.abs(action[1]) > 0.5) or (\n",
    "            not self.continuous and action in [1, 3]\n",
    "        ):\n",
    "            # Orientation engines\n",
    "            if self.continuous:\n",
    "                direction = np.sign(action[1])\n",
    "                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)\n",
    "                assert s_power >= 0.5 and s_power <= 1.0\n",
    "            else:\n",
    "                direction = action - 2\n",
    "                s_power = 1.0\n",
    "            ox = tip[0] * dispersion[0] + side[0] * (\n",
    "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
    "            )\n",
    "            oy = -tip[1] * dispersion[0] - side[1] * (\n",
    "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
    "            )\n",
    "            impulse_pos = (\n",
    "                self.lander.position[0] + ox - tip[0] * 17 / SCALE,\n",
    "                self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT / SCALE,\n",
    "            )\n",
    "            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)\n",
    "            p.ApplyLinearImpulse(\n",
    "                (ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),\n",
    "                impulse_pos,\n",
    "                True,\n",
    "            )\n",
    "            self.lander.ApplyLinearImpulse(\n",
    "                (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n",
    "                impulse_pos,\n",
    "                True,\n",
    "            )\n",
    "\n",
    "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
    "\n",
    "        pos = self.lander.position\n",
    "        vel = self.lander.linearVelocity\n",
    "        state = [\n",
    "            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2),\n",
    "            (pos.y - (self.helipad_y + LEG_DOWN / SCALE)) / (VIEWPORT_H / SCALE / 2),\n",
    "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS,\n",
    "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS,\n",
    "            self.lander.angle,\n",
    "            20.0 * self.lander.angularVelocity / FPS,\n",
    "            1.0 if self.legs[0].ground_contact else 0.0,\n",
    "            1.0 if self.legs[1].ground_contact else 0.0,\n",
    "        ]\n",
    "        assert len(state) == 8\n",
    "\n",
    "        reward = 0\n",
    "        shaping = (\n",
    "            -100 * np.sqrt(state[0] * state[0] + state[1] * state[1])\n",
    "            - 100 * np.sqrt(state[2] * state[2] + state[3] * state[3])\n",
    "            - 100 * abs(state[4])\n",
    "            + 10 * state[6]\n",
    "            + 10 * state[7]\n",
    "        )  # And ten points for legs contact, the idea is if you\n",
    "        # lose contact again after landing, you get negative reward\n",
    "        if self.prev_shaping is not None:\n",
    "            reward = shaping - self.prev_shaping\n",
    "        self.prev_shaping = shaping\n",
    "\n",
    "        reward -= (\n",
    "            m_power * 0.30\n",
    "        )  # less fuel spent is better, about -30 for heuristic landing\n",
    "        reward -= s_power * 0.03\n",
    "\n",
    "        done = False\n",
    "        if self.game_over or abs(state[0]) >= 1.0:\n",
    "            done = True\n",
    "            reward = -100\n",
    "        if not self.lander.awake:\n",
    "            done = True\n",
    "            reward = +100\n",
    "        return np.array(state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        import pygame\n",
    "        from pygame import gfxdraw\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.screen = pygame.display.set_mode((VIEWPORT_W, VIEWPORT_H))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        self.surf = pygame.Surface(self.screen.get_size())\n",
    "\n",
    "        pygame.transform.scale(self.surf, (SCALE, SCALE))\n",
    "        pygame.draw.rect(self.surf, (255, 255, 255), self.surf.get_rect())\n",
    "\n",
    "        for obj in self.particles:\n",
    "            obj.ttl -= 0.15\n",
    "            obj.color1 = (\n",
    "                int(max(0.2, 0.15 + obj.ttl) * 255),\n",
    "                int(max(0.2, 0.5 * obj.ttl) * 255),\n",
    "                int(max(0.2, 0.5 * obj.ttl) * 255),\n",
    "            )\n",
    "            obj.color2 = (\n",
    "                int(max(0.2, 0.15 + obj.ttl) * 255),\n",
    "                int(max(0.2, 0.5 * obj.ttl) * 255),\n",
    "                int(max(0.2, 0.5 * obj.ttl) * 255),\n",
    "            )\n",
    "\n",
    "        self._clean_particles(False)\n",
    "\n",
    "        for p in self.sky_polys:\n",
    "            scaled_poly = []\n",
    "            for coord in p:\n",
    "                scaled_poly.append((coord[0] * SCALE, coord[1] * SCALE))\n",
    "            pygame.draw.polygon(self.surf, (0, 0, 0), scaled_poly)\n",
    "            gfxdraw.aapolygon(self.surf, scaled_poly, (0, 0, 0))\n",
    "\n",
    "        for obj in self.particles + self.drawlist:\n",
    "            for f in obj.fixtures:\n",
    "                trans = f.body.transform\n",
    "                if type(f.shape) is circleShape:\n",
    "                    pygame.draw.circle(\n",
    "                        self.surf,\n",
    "                        color=obj.color1,\n",
    "                        center=trans * f.shape.pos * SCALE,\n",
    "                        radius=f.shape.radius * SCALE,\n",
    "                    )\n",
    "                    pygame.draw.circle(\n",
    "                        self.surf,\n",
    "                        color=obj.color2,\n",
    "                        center=trans * f.shape.pos * SCALE,\n",
    "                        radius=f.shape.radius * SCALE,\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    path = [trans * v * SCALE for v in f.shape.vertices]\n",
    "                    pygame.draw.polygon(self.surf, color=obj.color1, points=path)\n",
    "                    gfxdraw.aapolygon(self.surf, path, obj.color1)\n",
    "                    pygame.draw.aalines(\n",
    "                        self.surf, color=obj.color2, points=path, closed=True\n",
    "                    )\n",
    "\n",
    "                for x in [self.helipad_x1, self.helipad_x2]:\n",
    "                    x = x * SCALE\n",
    "                    flagy1 = self.helipad_y * SCALE\n",
    "                    flagy2 = flagy1 + 50\n",
    "                    pygame.draw.line(\n",
    "                        self.surf,\n",
    "                        color=(255, 255, 255),\n",
    "                        start_pos=(x, flagy1),\n",
    "                        end_pos=(x, flagy2),\n",
    "                        width=1,\n",
    "                    )\n",
    "                    pygame.draw.polygon(\n",
    "                        self.surf,\n",
    "                        color=(204, 204, 0),\n",
    "                        points=[\n",
    "                            (x, flagy2),\n",
    "                            (x, flagy2 - 10),\n",
    "                            (x + 25, flagy2 - 5),\n",
    "                        ],\n",
    "                    )\n",
    "                    gfxdraw.aapolygon(\n",
    "                        self.surf,\n",
    "                        [(x, flagy2), (x, flagy2 - 10), (x + 25, flagy2 - 5)],\n",
    "                        (204, 204, 0),\n",
    "                    )\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "\n",
    "        if mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        if mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.surf)), axes=(1, 0, 2)\n",
    "            )\n",
    "        else:\n",
    "            return self.isopen\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False\n",
    "\n",
    "\n",
    "def heuristic(env, s):\n",
    "    \"\"\"\n",
    "    The heuristic for\n",
    "    1. Testing\n",
    "    2. Demonstration rollout.\n",
    "    Args:\n",
    "        env: The environment\n",
    "        s (list): The state. Attributes:\n",
    "                  s[0] is the horizontal coordinate\n",
    "                  s[1] is the vertical coordinate\n",
    "                  s[2] is the horizontal speed\n",
    "                  s[3] is the vertical speed\n",
    "                  s[4] is the angle\n",
    "                  s[5] is the angular speed\n",
    "                  s[6] 1 if first leg has contact, else 0\n",
    "                  s[7] 1 if second leg has contact, else 0\n",
    "    returns:\n",
    "         a: The heuristic to be fed into the step function defined above to determine the next step and reward.\n",
    "    \"\"\"\n",
    "\n",
    "    angle_targ = s[0] * 0.5 + s[2] * 1.0  # angle should point towards center\n",
    "    if angle_targ > 0.4:\n",
    "        angle_targ = 0.4  # more than 0.4 radians (22 degrees) is bad\n",
    "    if angle_targ < -0.4:\n",
    "        angle_targ = -0.4\n",
    "    hover_targ = 0.55 * np.abs(\n",
    "        s[0]\n",
    "    )  # target y should be proportional to horizontal offset\n",
    "\n",
    "    angle_todo = (angle_targ - s[4]) * 0.5 - (s[5]) * 1.0\n",
    "    hover_todo = (hover_targ - s[1]) * 0.5 - (s[3]) * 0.5\n",
    "\n",
    "    if s[6] or s[7]:  # legs have contact\n",
    "        angle_todo = 0\n",
    "        hover_todo = (\n",
    "            -(s[3]) * 0.5\n",
    "        )  # override to reduce fall speed, that's all we need after contact\n",
    "\n",
    "    if env.continuous:\n",
    "        a = np.array([hover_todo * 20 - 1, -angle_todo * 20])\n",
    "        a = np.clip(a, -1, +1)\n",
    "    else:\n",
    "        a = 0\n",
    "        if hover_todo > np.abs(angle_todo) and hover_todo > 0.05:\n",
    "            a = 2\n",
    "        elif angle_todo < -0.05:\n",
    "            a = 3\n",
    "        elif angle_todo > +0.05:\n",
    "            a = 1\n",
    "    return a\n",
    "\n",
    "\n",
    "def demo_heuristic_lander(env, seed=None, render=False):\n",
    "\n",
    "    # wind power must be reduced for heuristic landing\n",
    "    env.wind_power = 0.2\n",
    "\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    s = env.reset(seed=seed)\n",
    "    while True:\n",
    "        a = heuristic(env, s)\n",
    "        s, r, done, info = env.step(a)\n",
    "        total_reward += r\n",
    "\n",
    "        if render:\n",
    "            still_open = env.render()\n",
    "            if still_open == False:\n",
    "                break\n",
    "\n",
    "        if steps % 20 == 0 or done:\n",
    "            print(\"observations:\", \" \".join([f\"{x:+0.2f}\" for x in s]))\n",
    "            print(f\"step {steps} total_reward {total_reward:+0.2f}\")\n",
    "        steps += 1\n",
    "        if done:\n",
    "            break\n",
    "    if render:\n",
    "        env.close()\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "class LunarLanderContinuous:\n",
    "    def __init__(self):\n",
    "        raise error.Error(\n",
    "            \"Error initializing LunarLanderContinuous Environment.\\n\"\n",
    "            \"Currently, we do not support initializing this mode of environment by calling the class directly.\\n\"\n",
    "            \"To use this environment, instead create it by specifying the continuous keyword in gym.make, i.e.\\n\"\n",
    "            'gym.make(\"LunarLander-v2\", continuous=True)'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 128)\n",
    "        self.linear2 = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, state):\n",
    "        if cuda == False: \n",
    "            state = torch.from_numpy(state).float()\n",
    "        output_1 = F.relu(self.linear1(state))\n",
    "        output_2 = self.linear2(output_1)\n",
    "\n",
    "        distribution = Categorical(F.softmax(output_2, dim=-1))\n",
    "\n",
    "        return output_1, distribution\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.linear1(state)\n",
    "\n",
    "        return value\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, actor, critic):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "\n",
    "        self.logprobs = []\n",
    "        self.state_values = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, state):\n",
    "        state, distribution = self.actor(state)\n",
    "        action = distribution.sample()\n",
    "        \n",
    "        state_value = self.critic(state)\n",
    "        \n",
    "        self.logprobs.append(distribution.log_prob(action))\n",
    "        self.state_values.append(state_value)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def calculateLoss(self, gamma=0.99):\n",
    "        # calculating discounted rewards:\n",
    "        rewards = []\n",
    "        dis_reward = 0\n",
    "        for reward in self.rewards[::-1]:\n",
    "            dis_reward = reward + gamma * dis_reward\n",
    "            rewards.insert(0, dis_reward)\n",
    "                \n",
    "        # normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std())\n",
    "        \n",
    "        loss = 0\n",
    "        for logprob, value, reward in zip(self.logprobs, self.state_values, rewards):\n",
    "            advantage = reward  - value.item()\n",
    "            action_loss = -logprob * advantage\n",
    "            if cuda: reward = reward.cuda()\n",
    "            value_loss = F.smooth_l1_loss(value, reward)\n",
    "            loss += (action_loss + value_loss)   \n",
    "        return loss\n",
    "\n",
    "    def clearMemory(self):\n",
    "        del self.logprobs[:]\n",
    "        del self.state_values[:]\n",
    "        del self.rewards[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cuda = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, optimizer, episodes = 2000, gamma = 0.99, is_render = True):\n",
    "    avg_rewards,  episode_rewards= [], []\n",
    "    \n",
    "    for episode in  range(episodes):\n",
    "        state = env.reset(seed = 42)\n",
    "        total_reward = 0\n",
    "\n",
    "        for i in range(10000):\n",
    "            if cuda: state = torch.FloatTensor(state).to(device)\n",
    "            action = agent(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            agent.rewards.append(reward)\n",
    "            total_reward += reward\n",
    "            if is_render and episode % 100 == 0:\n",
    "                env.render()\n",
    "            if done:\n",
    "                episode_rewards.append(total_reward)\n",
    "                break\n",
    "\n",
    "        # Updating the policy :\n",
    "        optimizer.zero_grad()\n",
    "        loss = agent.calculateLoss(gamma)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        agent.clearMemory()\n",
    "        \n",
    "        if episode >= 100:\n",
    "          avg_rewards.append(np.mean(episode_rewards[-100:]))\n",
    "          if avg_rewards[-1] >= 200:\n",
    "            print(\"Solved on episode \",episode)\n",
    "            return episode_rewards, avg_rewards\n",
    "\n",
    "        if episode % 100 == 0 and episode >= 100:\n",
    "            print(\"e: %d, episode reward: %.2f, avg reward %.2f\" % (episode, episode_rewards[-1], avg_rewards[-1]))\n",
    "\n",
    "    return episode_rewards, avg_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviorment = LunarLander(gravity=-10.0)\n",
    "\n",
    "lr = 0.02\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "agent = ActorCritic(actor, critic)\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\55366\\AppData\\Local\\Temp\\ipykernel_70368\\647289630.py:65: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.smooth_l1_loss(value, reward)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 100, episode reward: -402.00, avg reward -428.68\n"
     ]
    }
   ],
   "source": [
    "rew1, avg1 = train(enviorment, agent, optimizer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4e5af365abde9565455761093e8e9bc7189c8b509b33061fd0f90eaa1b4d4cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
